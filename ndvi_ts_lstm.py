"""
NDVI Time Series Analysis and Forecasting System
==============================================

–ú–æ–¥—É–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ NDVI —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LSTM –∏ multi-head attention.

–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
- DataManager: —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏ NDVI –∏ –ø–æ–≥–æ–¥—ã
- ModelTrainer: –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π
- NDVIPredictor: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
- ConfigManager: —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
- DebugLogger: —Å–∏—Å—Ç–µ–º–∞ –æ—Ç–ª–∞–¥–∫–∏
"""

import os
import json
import warnings
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime

import ee
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.preprocessing import MinMaxScaler
from scipy.interpolate import UnivariateSpline
import plotly.graph_objects as go
import openmeteo_requests
import requests_cache
from retry_requests import retry

warnings.filterwarnings("ignore")

# ===================== –ö–û–ù–°–¢–ê–ù–¢–´ =====================
@dataclass
class ModelConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
    LSTM_UNITS: int = 244
    NUM_LAYERS: int = 1
    DROPOUT_RATE: float = 0.2887856831106061
    LEARNING_RATE: float = 0.001827795604676652
    BATCH_SIZE: int = 128
    EPOCHS: int = 200
    WEIGHT_DECAY: float = 1e-5
    ATTENTION_HEADS: int = 4

@dataclass
class DataConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
    RESAMPLE_FREQUENCY: str = '5D'
    CLOUD_PERCENTAGE_THRESHOLD: int = 70
    SCALE_FACTOR: int = 10000
    TEMP_MIN_RANGE: Tuple[float, float] = (-60, 60)
    TEMP_MAX_RANGE: Tuple[float, float] = (-60, 60)
    HUMIDITY_RANGE: Tuple[float, float] = (0, 100)
    PRECIPITATION_MIN: float = 0

WEIGHTS_DIR = "weights"
CONFIG_FILE = "configs/config_ndvi.json"
CACHE_FILE = ".cache"
RESULTS_DIR = "results"  # –ò–∑–º–µ–Ω–µ–Ω–æ —Å images –Ω–∞ results
SERVICE_ACCOUNT_FILE = "key.json"

# ===================== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ö–õ–ê–°–°–´ =====================

class DebugLogger:
    """–°–∏—Å—Ç–µ–º–∞ –æ—Ç–ª–∞–¥–∫–∏ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    @staticmethod
    def log_data_shape(name: str, data: Any) -> None:
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Ñ–æ—Ä–º—É –¥–∞–Ω–Ω—ã—Ö"""
        if hasattr(data, 'shape'):
            print(f"üìä {name}: shape = {data.shape}")
        elif isinstance(data, (list, tuple)):
            print(f"üìä {name}: length = {len(data)}")
        elif isinstance(data, pd.DataFrame):
            print(f"üìä {name}: DataFrame shape = {data.shape}, columns = {list(data.columns)}")
        else:
            print(f"üìä {name}: type = {type(data)}")
    
    @staticmethod
    def log_ndvi_stats(ndvi_values: List[float], source: str = "API") -> None:
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É NDVI"""
        if not ndvi_values:
            print(f"‚ö†Ô∏è  {source}: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö NDVI")
            return
            
        min_val, max_val = min(ndvi_values), max(ndvi_values)
        mean_val = np.mean(ndvi_values)
        
        print(f"üåø {source} NDVI —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   üìà –î–∏–∞–ø–∞–∑–æ–Ω: {min_val:.3f} - {max_val:.3f}")
        print(f"   üìä –°—Ä–µ–¥–Ω–µ–µ: {mean_val:.3f}")
        print(f"   üìã –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(ndvi_values)}")
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        if max_val < 0:
            print("   ‚ö†Ô∏è  –í—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ - –ø—Ä–æ–±–ª–µ–º–Ω–∞—è –æ–±–ª–∞—Å—Ç—å!")
        elif max_val > 0.8:
            print("   ‚úÖ –í—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è - –∑–¥–æ—Ä–æ–≤–∞—è —Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
        elif max_val > 0.3:
            print("   ‚úÖ –£–º–µ—Ä–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è - –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
        else:
            print("   ‚ö†Ô∏è  –ù–∏–∑–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è - —Å–ª–∞–±–∞—è —Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
    
    @staticmethod
    def log_training_progress(epoch: int, total_epochs: int, loss: float, model_name: str) -> None:
        """–õ–æ–≥–∏—Ä—É–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è"""
        if (epoch + 1) % 20 == 0 or epoch + 1 == total_epochs:
            progress = (epoch + 1) / total_epochs * 100
            print(f"üöÄ {model_name}: Epoch [{epoch+1}/{total_epochs}] ({progress:.1f}%) - Loss: {loss:.6f}")

class ConfigManager:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""
    
    @staticmethod
    def load_config(config_path: str = CONFIG_FILE) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                config = json.load(f)
            
            print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {config_path}")
            print(f"üìç –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã: {len(config['coordinates'])} —Ç–æ—á–µ–∫")
            print(f"üìÖ –ü–µ—Ä–∏–æ–¥: {config['start_date']} - {config['end_date']}")
            print(f"üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: n_steps_in={config['n_steps_in']}, n_steps_out={config['n_steps_out']}")
            
            return config
        except FileNotFoundError:
            raise FileNotFoundError(f"–§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ {config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        except json.JSONDecodeError:
            raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON –≤ —Ñ–∞–π–ª–µ {config_path}")
    
    @staticmethod
    def validate_config(config: Dict[str, Any]) -> None:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
        required_keys = ['coordinates', 'start_date', 'end_date', 'n_steps_in', 'n_steps_out']
        missing_keys = [key for key in required_keys if key not in config]
        
        if missing_keys:
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {missing_keys}")
        
        if config['n_steps_in'] <= 0 or config['n_steps_out'] <= 0:
            raise ValueError("n_steps_in –∏ n_steps_out –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º–∏")
        
        print("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤–∞–ª–∏–¥–Ω–∞")

# ===================== –ú–û–î–ï–õ–¨ LSTM =====================

class LSTMModel(nn.Module):
    """LSTM –º–æ–¥–µ–ª—å —Å multi-head attention"""
    
    def __init__(self, input_size: int, config: ModelConfig):
        super(LSTMModel, self).__init__()
        self.hidden_size = config.LSTM_UNITS
        self.num_layers = config.NUM_LAYERS
        
        # LSTM —Å–ª–æ–π
        self.lstm = nn.LSTM(
            input_size, 
            self.hidden_size, 
            self.num_layers, 
            batch_first=True, 
            dropout=config.DROPOUT_RATE if self.num_layers > 1 else 0
        )
        
        # Multi-head attention
        self.attention = nn.MultiheadAttention(
            self.hidden_size, 
            num_heads=config.ATTENTION_HEADS
        )
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        self.layer_norm = nn.LayerNorm(self.hidden_size)
        self.dropout = nn.Dropout(config.DROPOUT_RATE)
        self.fc = nn.Linear(self.hidden_size, 1)  # –í—ã—Ö–æ–¥ —Ç–æ–ª—å–∫–æ 1 –∑–Ω–∞—á–µ–Ω–∏–µ
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        batch_size, seq_len = x.size(0), x.size(1)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)
        
        # LSTM
        lstm_out, _ = self.lstm(x, (h0, c0))
        
        # Attention (seq_len, batch, hidden)
        attn_input = lstm_out.permute(1, 0, 2)
        attn_output, _ = self.attention(
            query=attn_input,
            key=attn_input,
            value=attn_input
        )
        
        # Residual connection + layer norm
        attn_output = attn_output.permute(1, 0, 2)  # (batch, seq, hidden)
        output = self.layer_norm(lstm_out + attn_output)
        output = self.dropout(output)
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (—Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —à–∞–≥)
        return self.fc(output[:, -1, :])

# ===================== –£–ü–†–ê–í–õ–ï–ù–ò–ï –î–ê–ù–ù–´–ú–ò =====================

class DataManager:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏ NDVI –∏ –ø–æ–≥–æ–¥—ã"""
    
    def __init__(self, coordinates: List[Tuple[float, float]], service_account_file: str = SERVICE_ACCOUNT_FILE):
        self.coordinates = coordinates
        self.service_account_file = service_account_file
        self._initialize_ee()
        self._setup_weather_client()
        
        # Scalers
        self.scaler_x = MinMaxScaler()
        self.scaler_y = MinMaxScaler()
        self.scaler_y_smoothed = MinMaxScaler()
        
    def _initialize_ee(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Google Earth Engine"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –∫–ª—é—á–∞
            if not os.path.exists(self.service_account_file):
                raise FileNotFoundError(
                    f"–§–∞–π–ª –∫–ª—é—á–∞ {self.service_account_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. "
                    f"–°–∫–æ–ø–∏—Ä—É–π—Ç–µ key.json.example –≤ key.json –∏ –∑–∞–ø–æ–ª–Ω–∏—Ç–µ —Å–≤–æ–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏."
                )
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º credentials –∏–∑ JSON —Ñ–∞–π–ª–∞
            with open(self.service_account_file, 'r') as f:
                service_account_info = json.load(f)
            
            credentials = ee.ServiceAccountCredentials(
                service_account_info["client_email"], 
                self.service_account_file
            )
            ee.Initialize(credentials)
            print("‚úÖ Google Earth Engine –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –∫–ª—é—á–æ–º –∏–∑ —Ñ–∞–π–ª–∞")
            
        except FileNotFoundError as e:
            print(f"‚ùå {e}")
            print("üí° –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª key.json –Ω–∞ –æ—Å–Ω–æ–≤–µ key.json.example")
            raise
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ GEE: {e}")
            print("üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –≤ key.json")
            raise
    
    def _setup_weather_client(self) -> None:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞ –ø–æ–≥–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        cache_session = requests_cache.CachedSession(CACHE_FILE, expire_after=3600)
        retry_session = retry(cache_session, retries=5, backoff_factor=0.2)
        self.weather_client = openmeteo_requests.Client(session=retry_session)
        print("‚úÖ –ö–ª–∏–µ–Ω—Ç –ø–æ–≥–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
    
    def get_ndvi_data(self, start_date: str, end_date: str) -> pd.DataFrame:
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ NDVI —á–µ—Ä–µ–∑ Google Earth Engine"""
        print(f"üõ∞Ô∏è  –ü–æ–ª—É—á–µ–Ω–∏–µ NDVI –¥–∞–Ω–Ω—ã—Ö: {start_date} - {end_date}")
        
        aoi = ee.Geometry.Polygon([self.coordinates])
        collection = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
                     .filterDate(start_date, end_date)
                     .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', DataConfig.CLOUD_PERCENTAGE_THRESHOLD))
                     .map(self._mask_clouds)
                     .map(self._calculate_ndvi))
        
        def compute_ndvi(image):
            values = image.reduceRegion(
                reducer=ee.Reducer.mean(),
                geometry=aoi,
                scale=10
            )
            
            return ee.Feature(None, {
                'date': ee.Date(image.get('system:time_start')).format('YYYY-MM-dd'),
                'NDVI': values.get('NDVI')
            })
        
        ndvi_collection = collection.filterBounds(aoi).select('NDVI').map(compute_ndvi)
        ndvi_info = ndvi_collection.getInfo()
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        dates, ndvi_values = [], []
        for feature in ndvi_info['features']:
            props = feature['properties']
            if props.get('date') and props.get('NDVI') is not None:
                dates.append(props['date'])
                ndvi_values.append(props['NDVI'])
        
        # –û—Ç–ª–∞–¥–∫–∞
        DebugLogger.log_ndvi_stats(ndvi_values, "GEE API")
        
        df = pd.DataFrame({
            'Date': pd.to_datetime(dates),
            'NDVI': ndvi_values
        })
        
        print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π NDVI")
        return df.sort_values('Date').reset_index(drop=True)
    
    def _mask_clouds(self, image):
        """–ú–∞—Å–∫–∏—Ä–æ–≤–∫–∞ –æ–±–ª–∞–∫–æ–≤"""
        qa = image.select('QA60')
        scl = image.select('SCL')
        cloud_mask = (qa.bitwiseAnd(1 << 10).eq(0)
                     .And(qa.bitwiseAnd(1 << 11).eq(0))
                     .And(scl.neq(3)).And(scl.neq(8)).And(scl.neq(9))
                     .And(scl.neq(10)).And(scl.neq(11)))
        
        return (image.updateMask(cloud_mask)
                    .divide(DataConfig.SCALE_FACTOR)
                    .select("B.*")
                    .copyProperties(image, ["system:time_start"]))
    
    def _calculate_ndvi(self, image):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ NDVI"""
        ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')
        return image.addBands(ndvi)
    
    def get_weather_data(self, start_date: str, end_date: str) -> pd.DataFrame:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–≥–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        print(f"üå§Ô∏è  –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–≥–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {start_date} - {end_date}")
        
        # –¶–µ–Ω—Ç—Ä –ø–æ–ª–∏–≥–æ–Ω–∞
        lats = [coord[0] for coord in self.coordinates]
        lons = [coord[1] for coord in self.coordinates]
        centroid_lat, centroid_lon = np.mean(lats), np.mean(lons)
        
        print(f"üìç –¶–µ–Ω—Ç—Ä –æ–±–ª–∞—Å—Ç–∏: {centroid_lat:.3f}, {centroid_lon:.3f}")
        
        params = {
            "latitude": centroid_lat,
            "longitude": centroid_lon,
            "start_date": start_date,
            "end_date": end_date,
            "daily": ["temperature_2m_min", "temperature_2m_max"],
            "hourly": ["temperature_2m", "relative_humidity_2m", "precipitation"]
        }
        
        url = "https://historical-forecast-api.open-meteo.com/v1/forecast"
        responses = self.weather_client.weather_api(url, params=params)
        response = responses[0]
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        weather_df = self._process_weather_response(response)
        weather_df = self._validate_weather_data(weather_df)
        
        print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(weather_df)} –∑–∞–ø–∏—Å–µ–π –ø–æ–≥–æ–¥—ã")
        DebugLogger.log_data_shape("Weather data", weather_df)
        
        return weather_df
    
    def _process_weather_response(self, response) -> pd.DataFrame:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç –ø–æ–≥–æ–¥–Ω–æ–≥–æ API"""
        # –î–Ω–µ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        daily = response.Daily()
        daily_dates = pd.date_range(
            start=pd.to_datetime(daily.Time(), unit="s", utc=True),
            end=pd.to_datetime(daily.TimeEnd(), unit="s", utc=True),
            freq=pd.Timedelta(seconds=daily.Interval()),
            inclusive="left"
        )
        
        daily_df = pd.DataFrame({
            "Date": daily_dates,
            "TempMin": daily.Variables(0).ValuesAsNumpy(),
            "TempMax": daily.Variables(1).ValuesAsNumpy()
        })
        daily_df["Date"] = daily_df["Date"].dt.tz_localize(None)
        
        # –ü–æ—á–∞—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        hourly = response.Hourly()
        hourly_dates = pd.date_range(
            start=pd.to_datetime(hourly.Time(), unit="s", utc=True),
            end=pd.to_datetime(hourly.TimeEnd(), unit="s", utc=True),
            freq=pd.Timedelta(seconds=hourly.Interval()),
            inclusive="left"
        )
        
        hourly_df = pd.DataFrame({
            "DateTime": hourly_dates,
            "temperature_2m": hourly.Variables(0).ValuesAsNumpy(),
            "relative_humidity_2m": hourly.Variables(1).ValuesAsNumpy(),
            "precipitation": hourly.Variables(2).ValuesAsNumpy()
        })
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –¥–Ω—è–º
        daily_hourly = hourly_df.groupby(hourly_df["DateTime"].dt.date).agg({
            "temperature_2m": "mean",
            "relative_humidity_2m": "mean",
            "precipitation": "sum"
        }).reset_index()
        daily_hourly["Date"] = pd.to_datetime(daily_hourly["DateTime"]).dt.tz_localize(None)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
        weather_df = pd.merge(daily_df, daily_hourly, on="Date", how="inner")
        weather_df = weather_df.rename(columns={
            "relative_humidity_2m": "RelativeHumidity",
            "precipitation": "Precipitation"
        })
        
        return weather_df[["Date", "TempMin", "TempMax", "RelativeHumidity", "Precipitation"]]
    
    def _validate_weather_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –ø–æ–≥–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        initial_count = len(df)
        
        df = df[
            (df['TempMax'] >= DataConfig.TEMP_MAX_RANGE[0]) & 
            (df['TempMax'] <= DataConfig.TEMP_MAX_RANGE[1]) &
            (df['TempMin'] >= DataConfig.TEMP_MIN_RANGE[0]) & 
            (df['TempMin'] <= DataConfig.TEMP_MIN_RANGE[1]) &
            (df['RelativeHumidity'] >= DataConfig.HUMIDITY_RANGE[0]) & 
            (df['RelativeHumidity'] <= DataConfig.HUMIDITY_RANGE[1]) &
            (df['Precipitation'] >= DataConfig.PRECIPITATION_MIN)
        ]
        
        filtered_count = initial_count - len(df)
        if filtered_count > 0:
            print(f"‚ö†Ô∏è  –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {filtered_count} –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –ø–æ–≥–æ–¥—ã")
        
        return df
    
    def process_data(self, ndvi_df: pd.DataFrame, weather_df: pd.DataFrame, 
                    percentile: float, bimonthly_period: str, 
                    spline_smoothing: float) -> pd.DataFrame:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ NDVI –∏ –ø–æ–≥–æ–¥—ã"""
        print("üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è NDVI
        ndvi_df = self._interpolate_data(ndvi_df)
        DebugLogger.log_data_shape("NDVI –ø–æ—Å–ª–µ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏", ndvi_df)
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤—ã–±—Ä–æ—Å–æ–≤
        ndvi_df = self._filter_outliers(ndvi_df, percentile, bimonthly_period)
        DebugLogger.log_data_shape("NDVI –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏", ndvi_df)
        
        # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è
        ndvi_df = self._interpolate_data(ndvi_df)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ø–æ–≥–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        merged_df = pd.merge_asof(
            ndvi_df.sort_values('Date'), 
            weather_df.sort_values('Date'), 
            on='Date', 
            direction='nearest'
        )
        
        # –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ NDVI
        merged_df['NDVI_Smoothed'] = self._smooth_data(merged_df, spline_smoothing)
        
        print(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(merged_df)} –∑–∞–ø–∏—Å–µ–π")
        DebugLogger.log_data_shape("–§–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ", merged_df)
        
        return merged_df
    
    def _interpolate_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —à–∞–≥–æ–º"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –¥–∞—Ç
        duplicates_count = df['Date'].duplicated().sum()
        if duplicates_count > 0:
            print(f"üîç –ù–∞–π–¥–µ–Ω–æ {duplicates_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –¥–∞—Ç, —É–¥–∞–ª—è–µ–º...")
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –¥–∞—Ç, –æ—Å—Ç–∞–≤–ª—è—è –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
        df_clean = df.drop_duplicates(subset=['Date'], keep='first')
        
        df_indexed = df_clean.set_index('Date')
        df_resampled = df_indexed.resample(DataConfig.RESAMPLE_FREQUENCY).interpolate(method='linear')
        return df_resampled.reset_index()
    
    def _filter_outliers(self, df: pd.DataFrame, percentile: float, period: str) -> pd.DataFrame:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤—ã–±—Ä–æ—Å–æ–≤ –ø–æ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—é"""
        df = df.copy()
        df['Period'] = df['Date'].dt.to_period(period)
        
        thresholds = df.groupby('Period')['NDVI'].quantile(percentile/100).reset_index()
        df = df.merge(thresholds, on='Period', suffixes=('', '_threshold'))
        
        filtered_df = df[df['NDVI'] >= df['NDVI_threshold']]
        removed_count = len(df) - len(filtered_df)
        
        if removed_count > 0:
            print(f"üîç –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {removed_count} –≤—ã–±—Ä–æ—Å–æ–≤ ({percentile}% –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å)")
        
        return filtered_df.drop(columns=['Period', 'NDVI_threshold'])
    
    def _smooth_data(self, df: pd.DataFrame, smoothing: float) -> np.ndarray:
        """–°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å–ø–ª–∞–π–Ω–æ–º"""
        if len(df) < 4:
            print("‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è")
            return df['NDVI'].values
        
        x_ordinal = df['Date'].map(pd.Timestamp.toordinal)
        spline = UnivariateSpline(x_ordinal, df['NDVI'], s=smoothing)
        return spline(x_ordinal)
    
    def prepare_sequences(self, df: pd.DataFrame, n_steps_in: int, n_steps_out: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, int]:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        print(f"üì¶ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {n_steps_in} –≤—Ö–æ–¥–Ω—ã—Ö ‚Üí {n_steps_out} –≤—ã—Ö–æ–¥–Ω—ã—Ö")
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        features = ['TempMin', 'TempMax', 'RelativeHumidity', 'Precipitation']
        X_scaled = self.scaler_x.fit_transform(df[features])
        y_scaled = self.scaler_y.fit_transform(df[['NDVI']])
        y_smoothed_scaled = self.scaler_y_smoothed.fit_transform(df[['NDVI_Smoothed']])
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        X, y_original, y_smoothed = [], [], []
        
        for i in range(len(df) - n_steps_in - n_steps_out + 1):
            # –í—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–ø—Ä–∏–∑–Ω–∞–∫–∏ + NDVI)
            input_seq = np.hstack([
                X_scaled[i:i+n_steps_in],
                y_scaled[i:i+n_steps_in]
            ])
            X.append(input_seq)
            
            # –í—ã—Ö–æ–¥–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            y_original.append(y_scaled[i+n_steps_in:i+n_steps_in+n_steps_out].flatten())
            y_smoothed.append(y_smoothed_scaled[i+n_steps_in:i+n_steps_in+n_steps_out].flatten())
        
        X = np.array(X)
        y_original = np.array(y_original)
        y_smoothed = np.array(y_smoothed)
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {len(X)}")
        DebugLogger.log_data_shape("X (–≤—Ö–æ–¥—ã)", X)
        DebugLogger.log_data_shape("y_original", y_original)
        DebugLogger.log_data_shape("y_smoothed", y_smoothed)
        
        return X, y_original, y_smoothed, X.shape[2]

# ===================== –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô =====================

class ModelTrainer:
    """–û–±—É—á–µ–Ω–∏–µ LSTM –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"üñ•Ô∏è  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {self.device}")
        
        if self.device.type == 'cuda':
            print(f"üöÄ GPU: {torch.cuda.get_device_name(0)}")
            print(f"üíæ –ü–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    def create_model(self, input_size: int) -> LSTMModel:
        """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å LSTM"""
        model = LSTMModel(input_size, self.config).to(self.device)
        
        # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"üß† –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞:")
        print(f"   üìä –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
        print(f"   üéØ –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,}")
        
        return model
    
    def train_model(self, model: LSTMModel, X: np.ndarray, y: np.ndarray, model_name: str) -> LSTMModel:
        """–û–±—É—á–∞–µ—Ç –æ–¥–Ω—É –º–æ–¥–µ–ª—å"""
        print(f"\nüöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {model_name}")
        print(f"üìä –î–∞–Ω–Ω—ã–µ: {X.shape[0]} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_tensor = torch.FloatTensor(X).to(self.device)
        y_tensor = torch.FloatTensor(y).to(self.device)
        
        dataset = TensorDataset(X_tensor, y_tensor)
        dataloader = DataLoader(
            dataset, 
            batch_size=self.config.BATCH_SIZE, 
            shuffle=True,
            num_workers=0  # –î–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        )
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        criterion = nn.MSELoss()
        optimizer = optim.Adam(
            model.parameters(), 
            lr=self.config.LEARNING_RATE,
            weight_decay=self.config.WEIGHT_DECAY
        )
        
        # –û–±—É—á–µ–Ω–∏–µ
        model.train()
        loss_history = []
        
        for epoch in range(self.config.EPOCHS):
            epoch_loss = 0.0
            batch_count = 0
            
            for batch_X, batch_y in dataloader:
                optimizer.zero_grad()
                
                # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
                outputs = model(batch_X)
                
                # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (—É—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –≤—ã—Ö–æ–¥–Ω—ã–º —à–∞–≥–∞–º)
                loss = criterion(outputs.squeeze(), batch_y.mean(dim=1))
                
                # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
                loss.backward()
                
                # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                epoch_loss += loss.item()
                batch_count += 1
            
            avg_loss = epoch_loss / batch_count
            loss_history.append(avg_loss)
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            DebugLogger.log_training_progress(epoch, self.config.EPOCHS, avg_loss, model_name)
        
        print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ {model_name} –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –§–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ—Ç–µ—Ä—è: {loss_history[-1]:.6f}")
        
        model.eval()
        return model
    
    def save_models(self, model_original: LSTMModel, model_smoothed: LSTMModel) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–µ—Å–∞ –º–æ–¥–µ–ª–µ–π"""
        os.makedirs(WEIGHTS_DIR, exist_ok=True)
        
        original_path = os.path.join(WEIGHTS_DIR, "model_weights_original.pth")
        smoothed_path = os.path.join(WEIGHTS_DIR, "model_weights_filtered.pth")
        
        torch.save(model_original.state_dict(), original_path)
        torch.save(model_smoothed.state_dict(), smoothed_path)
        
        print(f"üíæ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"   üìÅ Original: {original_path}")
        print(f"   üìÅ Smoothed: {smoothed_path}")

# ===================== –ì–õ–ê–í–ù–´–ô –ö–õ–ê–°–° =====================

class NDVIForecaster:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è NDVI"""
    
    def __init__(self, config_path: str = CONFIG_FILE):
        print("üåø –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è NDVI")
        print("=" * 60)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.config = ConfigManager.load_config(config_path)
        ConfigManager.validate_config(self.config)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        self.coordinates = [tuple(coord) for coord in self.config["coordinates"]]
        self.start_date = self.config["start_date"]
        self.end_date = self.config["end_date"]
        self.n_steps_in = self.config["n_steps_in"]
        self.n_steps_out = self.config["n_steps_out"]
        self.percentile = self.config.get("percentile", 40)
        self.bimonthly_period = self.config.get("bimonthly_period", "2M")
        self.spline_smoothing = self.config.get("spline_smoothing", 0.7)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.data_manager = DataManager(self.coordinates)
        self.model_trainer = ModelTrainer(ModelConfig())
        
        # –î–∞–Ω–Ω—ã–µ –∏ –º–æ–¥–µ–ª–∏
        self.merged_df = None
        self.model_original = None
        self.model_filtered = None
        
        print("‚úÖ –°–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        print("=" * 60)
    
    def load_and_process_data(self) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ"""
        print("\nüì° –ó–ê–ì–†–£–ó–ö–ê –ò –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•")
        print("=" * 40)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        ndvi_df = self.data_manager.get_ndvi_data(self.start_date, self.end_date)
        weather_df = self.data_manager.get_weather_data(self.start_date, self.end_date)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
        self.merged_df = self.data_manager.process_data(
            ndvi_df, weather_df, 
            self.percentile, self.bimonthly_period, self.spline_smoothing
        )
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        print(f"\nüìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ê–ù–ù–´–•:")
        print(f"   üìÖ –ü–µ—Ä–∏–æ–¥: {self.merged_df['Date'].min()} - {self.merged_df['Date'].max()}")
        print(f"   üìã –ó–∞–ø–∏—Å–µ–π: {len(self.merged_df)}")
        
        ndvi_stats = self.merged_df['NDVI'].describe()
        print(f"   üåø NDVI: min={ndvi_stats['min']:.3f}, max={ndvi_stats['max']:.3f}, mean={ndvi_stats['mean']:.3f}")
        
        smoothed_stats = self.merged_df['NDVI_Smoothed'].describe()
        print(f"   üåø NDVI (—Å–≥–ª–∞–∂): min={smoothed_stats['min']:.3f}, max={smoothed_stats['max']:.3f}, mean={smoothed_stats['mean']:.3f}")
    
    def train_models(self) -> None:
        """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª–∏"""
        if self.merged_df is None:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é load_and_process_data()")
        
        print("\nüß† –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
        print("=" * 40)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        X, y_original, y_smoothed, input_size = self.data_manager.prepare_sequences(
            self.merged_df, self.n_steps_in, self.n_steps_out
        )
        
        if len(X) == 0:
            raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        self.model_original = self.model_trainer.create_model(input_size)
        self.model_filtered = self.model_trainer.create_model(input_size)
        
        self.model_original = self.model_trainer.train_model(
            self.model_original, X, y_original, "Original"
        )
        self.model_filtered = self.model_trainer.train_model(
            self.model_filtered, X, y_smoothed, "Smoothed"
        )
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self.model_trainer.save_models(self.model_original, self.model_filtered)
    
    def run_training_pipeline(self) -> None:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è"""
        print("üöÄ –ó–ê–ü–£–°–ö –ü–û–õ–ù–û–ì–û –ü–ê–ô–ü–õ–ê–ô–ù–ê –û–ë–£–ß–ï–ù–ò–Ø")
        print("=" * 60)
        
        start_time = datetime.now()
        
        try:
            self.load_and_process_data()
            self.train_models()
            
            end_time = datetime.now()
            duration = end_time - start_time
            
            print("\n" + "=" * 60)
            print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
            print(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration}")
            print(f"üìÅ –í–µ—Å–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {WEIGHTS_DIR}/")
            print("=" * 60)
            
        except Exception as e:
            print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
            raise

# ===================== –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø =====================

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    try:
        forecaster = NDVIForecaster()
        forecaster.run_training_pipeline()
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\nüí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        raise

if __name__ == "__main__":
    main() 